# 01.入门

## 1.简介

Hadoop是一个开源的分布式计算平台，旨在处理大规模数据集的存储和处理。它提供了可靠性、可扩展性和容错性，使得能够在由成百上千台普通服务器组成得集群中进行并行计算和存储。

Hadoop的核心组件包括以下两个部分：

1. **Hadoop Distributed File System（HDFS）**：HDFS是Hadoop的分布式文件系统，专门用于存储大规模数据集。它可以将数据分布在集群的多个节点上，实现数据的冗余和高可靠性。HDFS采用了块存储的概念，将大文件切分多个块，并在集群中的多个节点上进行存储。这样的设计使得Hadoop可以高效地处理大规模数据，并且具备容错能力。

2. **Hadoop MapReduce**：MapReduce是Hadoop的编程模型和计算框架，用于对存储在HDFS上的数据进行分布式处理。MapReduce通过将计算任务分为Map和Reduce两个阶段来实现并行计算。Map阶段将数据切分为独立的子问题，并在集群中的不同节点上进行并行处理。然后，Reduce阶段将Map阶段的结果进行汇总和归约，生成最终的输出。MapReduce的设计思想使得可以在分布式环境下高效地处理大规模数据集。

除了HDFS和MapReduce，Hadoop生态系统还包含其他组件和工具，如Hive、Pig、HBase、Spark等，同于数据处理、数据存储和数据分析等不同的需求。

Hadoop的优势在于它的分布式性能、容错能力和可扩展性，使得能够处理具有PB级规模的数据，并在多台服务器上进行并行计算。这使得Hadoop成为大数据处理和分析的重要工具之一。

## 2.准备启动Hadoop集群

### 2.1.前置安装

#### 2.1.1.在Ubuntu Linux上

```shell
$ sudo apt-get install ssh
$ sudo apt-get install pdsh
```

### 2.2.下载

对应的java版本

| 版本      | 不兼容的更改                                                 | 相关的 JDK 错误系统工单                                      | 相关 JIRA                                                    |
| :-------- | :----------------------------------------------------------- | :----------------------------------------------------------- | :----------------------------------------------------------- |
| 1.8.0_242 | sun.nio.ch.SocketAdaptor 的可见性从公共更改为包私有。TestIPC#testRTEDuringConnectionSetup 受到影响。 | [JDK-8237177](https://bugs.openjdk.java.net/browse/JDK-8237177) | [![img](https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype)HADOOP-15787](https://issues.apache.org/jira/browse/HADOOP-15787) - [JDK11]TestIPC.testRTEDuringConnection设置失败 **解决** |
| 1.8.0_242 | 当客户端请求可续订票证并且 KDC 返回不可续订票证时，Kerberos Java 客户端将因“消息流已修改 （41）”而失败。如果您的委托人不被允许获得可续订的票证，您必须从您的 krb5.conf 中删除“renew_lifetime”设置。 | [JDK-8131051](https://bugs.openjdk.java.net/browse/JDK-8131051) |                                                              |
| 1.8.0_191 | 所有 DES 密码套件均已禁用。如果显式使用 DES 密码套件，则需要将密码套件更改为强密码套件。 |                                                              | [![img](https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype)HADOOP-16016](https://issues.apache.org/jira/browse/HADOOP-16016) - TestSSLFactory#testServerWeakCiphers在预提交构建中偶尔失败 **解决** |
| 1.8.0_171 | 在 Apache Hadoop 2.7.0 到 2.7.6、2.8.0 到 2.8.4、2.9.0 到 2.9.1、3.0.0 到 3.0.2 和 3.1.0 中，由于 [KeyStore 机制增强](https://www.oracle.com/technetwork/java/javase/8u171-relnotes-4308888.html#JDK-8189997)，KMS 因 java.security.UnrecoverableKeyException 而失败。您需要将系统属性“jceks.key.serialFilter”设置为以下值以避免此错误：`java.lang.Enum;java.security.KeyRep;java.security.KeyRep$Type;javax.crypto.spec.SecretKeySpec;org.apache.hadoop.crypto.key.JavaKeyStoreProvider$KeyMetadata;!*"` |                                                              | [![img](https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype)HADOOP-15473](https://issues.apache.org/jira/browse/HADOOP-15473) - 在密钥提供程序中配置串行筛选器以避免由JDK-8189997引起的不可恢复的密钥异常 **解决** |

[Hadoop下载地址](https://dlcdn.apache.org/hadoop/common/)

[JDK下载地址]([Java Archive Downloads - Java SE 8 (oracle.com)](https://www.oracle.com/java/technologies/javase/javase8-archive-downloads.html))

### 2.3.解压

```shell
$ sudo tar -zxvf hadoop-3.3.6.tar.gz
$ sudo tar -zxvf jdk-8u202-linux-x64.tar.gz
```

### 2.4.配置&运行

#### 修改配置

`[Hadoop安装目录]/etc/hadoop/hadoop-env.sh`

```shell
# set to the root of your Java installation
export JAVA_HOME=/usr/local/java/jdk1.8.0_202
```

#### 运行

```shell
$ bin/hadoop
```

### 2.5.部署

#### 2.5.1.单机

默认情况下，Hadoop配置为在非分布式模式下运行，作为单个Java进程。这对于调试很有用。

下面的示例复制解压缩的conf目录以用作输入，然后查找并现实给定正则表达式的每个匹配项。输出将写入给指定的输出目录。

```shell
$ mkdir input
$ cp etc/hadoop/*.xml input
$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar grep input output 'dfs[a-z.]+'
$ cat output/*
```

#### 2.5.2.伪分布式（未实践）

Hadoop也可以在伪分布式模式下的单节点上运行，其中每个Hadoop守护进程都在单独的Java进程中运行。

使用以下内容

##### 配置

`etc/hadoop/core-site.xml`

```xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>
```

`etc/hadoop/hdfs-site.xml`

```xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>
```

##### 设置无密码ssh

```shell
$ update-alternatives --list iptables
$ sudo update-alternatives --set iptables /usr/sbin/iptables-legacy

# 关闭防火墙
$ sudo ufw disable

# 安装openssh-server
$ sudo apt update
$ sudo apt install openssh-server

$ ssh localhost

# 如果没有密码，则无法通过ssh连接到本地主机
$ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
$ chmod 0600 ~/.ssh/authorized_keys
```

[Apache Hadoop 3.3.6 – Hadoop：设置单节点集群。](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html#Download)

